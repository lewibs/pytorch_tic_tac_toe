{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "Action = namedtuple('Action', (\"player\", \"move\"))\n",
    "Delta = namedtuple('Delta', (\"summary\", \"reward\"))\n",
    "\n",
    "REWARD = {\n",
    "    \"GAME_IS_OVER\": -100,\n",
    "    \"NOT_TURN\": -100,\n",
    "    \"TAKEN_SPACE\": -100,\n",
    "    \"TIE\": 5,\n",
    "    \"WIN\": 100,\n",
    "    \"BLOCKED_WIN\": 50, \n",
    "    \"TWO_IN_ROW\": 10,\n",
    "}\n",
    "\n",
    "class TicTacToe(gym.Env):\n",
    "    _no_player = 0\n",
    "    _player_1 = 1\n",
    "    _player_2 = 2\n",
    "    \n",
    "    reward_range = (-np.inf, np.inf)\n",
    "    observation_space = spaces.MultiDiscrete([2 for _ in range(0, 9 * 3)])\n",
    "    action_space = spaces.Discrete(9)\n",
    "\n",
    "    \"\"\"\n",
    "    Board looks like:\n",
    "    [0, 1, 2,\n",
    "     3, 4, 5,\n",
    "     6, 7, 8]\n",
    "    \"\"\"\n",
    "    winning_states = [\n",
    "        [0, 1, 2],\n",
    "        [3, 4, 5],\n",
    "        [6, 7, 8],\n",
    "        [0, 3, 6],\n",
    "        [1, 4, 7],\n",
    "        [2, 5, 8],\n",
    "        [0, 4, 8],\n",
    "        [2, 4, 6],\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = torch.zeros(9, 3)\n",
    "        self.board[:,0] = 1\n",
    "\n",
    "        self.summary = {\n",
    "            \"done\": False,\n",
    "            \"moves\":0,\n",
    "            \"player_turn\":self._player_1,\n",
    "            \"winner\":self._no_player,\n",
    "            \"board\":self.board\n",
    "        }\n",
    "\n",
    "        return self._one_hot_summary()\n",
    "\n",
    "    def isDone(self):\n",
    "        return self.summary[\"done\"]\n",
    "\n",
    "    def _one_hot_summary(self):\n",
    "        board = self._one_hot_board()\n",
    "        turn = self._one_hot_turn()\n",
    "        summary = torch.cat((self._one_hot_board(), self._one_hot_turn()), dim=0)\n",
    "        return summary.unsqueeze(0)\n",
    "\n",
    "    def _one_hot_board(self):\n",
    "        return self.board.flatten()\n",
    "    \n",
    "    def _one_hot_turn(self):\n",
    "        move = torch.zeros(1,2)\n",
    "        move[0][self.summary[\"player_turn\"] - 1] = 1\n",
    "        return move.flatten()\n",
    "    \n",
    "    def checkForWin(self):\n",
    "        for state in self.winning_states:\n",
    "            if all(self.board[:,self._player_1][index] == 1 for index in state):\n",
    "                return self._player_1\n",
    "            elif all(self.board[:,self._player_2][index] == 1 for index in state):\n",
    "                return self._player_2\n",
    "                \n",
    "        return self._no_player\n",
    "    \n",
    "    def hasTwoInRow(self, player):\n",
    "        player_1s = 0\n",
    "        player_2s = 0\n",
    "        for state in self.winning_states:\n",
    "            row_player_1 = sum(self.board[:,self._player_1][index] == 1 for index in state)\n",
    "            row_player_2 = sum(self.board[:,self._player_2][index] == 1 for index in state)\n",
    "            if row_player_1 == 2 and row_player_2 == 0:\n",
    "                player_1s += 1\n",
    "            elif row_player_2 == 2 and row_player_1 == 0:\n",
    "                player_2s += 1\n",
    "                \n",
    "        return [player_1s, player_2s][player - 1]\n",
    "\n",
    "    \n",
    "    def checkForTie(self):\n",
    "        if self.checkForWin() == self._no_player and self.board[:,0].sum() == 0:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "\n",
    "        #make sure that if the game is over no one plays\n",
    "        if self.summary[\"winner\"] != 0:\n",
    "            reward += REWARD[\"GAME_IS_OVER\"]\n",
    "        \n",
    "        #make sure that the player whos turn it is is the one making the move\n",
    "        if self.summary[\"player_turn\"] != action.player:\n",
    "            reward += REWARD[\"NOT_TURN\"]\n",
    "\n",
    "        current_player = self.summary[\"player_turn\"]\n",
    "        self.summary[\"moves\"] += 1\n",
    "        self.summary[\"player_turn\"] = self._player_1 if self.summary[\"player_turn\"] == self._player_2 else self._player_2        \n",
    "        other_player = self.summary[\"player_turn\"]\n",
    "\n",
    "\n",
    "        #Move has already been played\n",
    "        if self.board[:,self._no_player][action.move] != 1:\n",
    "            reward += REWARD[\"TAKEN_SPACE\"]\n",
    "        else:\n",
    "            oponent_2_in_rows_before_action = self.hasTwoInRow(other_player)\n",
    "            \n",
    "            self.board[:,action.player][action.move] = 1\n",
    "            self.board[:,self._no_player][action.move] = 0\n",
    "\n",
    "            oponent_2_in_rows_after_action = self.hasTwoInRow(other_player)\n",
    "            \n",
    "            if oponent_2_in_rows_after_action < oponent_2_in_rows_before_action:\n",
    "                reward += REWARD[\"BLOCKED_WIN\"]\n",
    "\n",
    "        if self.checkForTie():\n",
    "            self.summary[\"winner\"] = self._no_player\n",
    "            self.summary[\"player_turn\"] = self._no_player\n",
    "            self.summary[\"done\"] = True\n",
    "            reward += REWARD[\"TIE\"]\n",
    "\n",
    "        winner = self.checkForWin()\n",
    "        if winner != self._no_player:\n",
    "            self.summary[\"winner\"] = winner\n",
    "            self.summary[\"player_turn\"] = self._no_player\n",
    "            self.summary[\"done\"] = True\n",
    "            reward += REWARD[\"WIN\"]\n",
    "        \n",
    "        almostThrees = self.hasTwoInRow(current_player)\n",
    "        if almostThrees != 0:\n",
    "            reward += almostThrees * REWARD[\"TWO_IN_ROW\"]\n",
    "\n",
    "        #reward for doing something new?\n",
    "        return Delta(self._one_hot_summary(), reward)\n",
    "\n",
    "    def render(self):\n",
    "        board = [\" \"] * 9\n",
    "        \n",
    "        for i, value in enumerate(self.board):\n",
    "            if torch.argmax(value) == 1:\n",
    "                icon = \"X\"\n",
    "            elif torch.argmax(value) == 2:\n",
    "                icon = \"O\"\n",
    "            else:\n",
    "                icon = \" \"\n",
    "\n",
    "            board[i] = icon\n",
    "\n",
    "        # Print the tic-tac-toe grid\n",
    "        print(\"{}|{}|{}\\n-----\\n{}|{}|{}\\n-----\\n{}|{}|{}\".format(*board))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"Copied verbatim from the PyTorch DQN tutorial.\n",
    "\n",
    "    During training, observations from the replay memory are\n",
    "    sampled for policy learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    \"\"\"Policy model. Consists of a fully connected feedforward\n",
    "    NN with 3 hidden layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(3*9 + 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for the model.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            return  self.forward(state).max(-1).indices.view(1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def act(self, state):\n",
    "        index = input(\"Enter position on board (0-8): \")\n",
    "        return torch.tensor([[int(index)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = state[0][:-2].reshape(3, 3, 3)\n",
    "        open_spots = state[:, :, 0].reshape(-1)\n",
    "        # Get indices of open spots (where the value is 1)\n",
    "        open_indices = np.where(open_spots == 1)\n",
    "        max = len(open_indices[0])\n",
    "        idx = random.randint(0,max-1)\n",
    "        return torch.tensor([[open_indices[0][idx]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5000\n",
    "batch_size = 128\n",
    "gamma = 0.99\n",
    "eps_start = 1.0\n",
    "eps_end = 0.1\n",
    "eps_steps = 2000\n",
    "\n",
    "env = TicTacToe()\n",
    "state = env.reset()\n",
    "\n",
    "policy_net = DQNAgent()\n",
    "target_net = DQNAgent()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=1e-3)\n",
    "memory = ReplayMemory(1000)\n",
    "\n",
    "def optimize_model(\n",
    "    optimizer: optim.Optimizer,\n",
    "    policy: DQNAgent,\n",
    "    target: DQNAgent,\n",
    "    memory: ReplayMemory,\n",
    "    batch_size: int,\n",
    "    gamma: float,\n",
    "):\n",
    "    \"\"\"Model optimization step, copied verbatim from the Torch DQN tutorial.\n",
    "    \n",
    "    Arguments:\n",
    "        device {torch.device} -- Device\n",
    "        optimizer {torch.optim.Optimizer} -- Optimizer\n",
    "        policy {Policy} -- Policy net\n",
    "        target {Policy} -- Target net\n",
    "        memory {ReplayMemory} -- Replay memory\n",
    "        batch_size {int} -- Number of observations to use per batch step\n",
    "        gamma {float} -- Reward discount factor\n",
    "    \"\"\"\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        dtype=torch.bool,\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy\n",
    "    state_action_values = policy(torch.zeros(29))\n",
    "    state_action_values = policy(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(batch_size)\n",
    "    next_state_values[non_final_mask] = target(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(\n",
    "        state_action_values, expected_state_action_values.unsqueeze(1)\n",
    "    )\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "def select_dummy_action(state: np.array) -> int:\n",
    "    return RandomAgent().act(state)\n",
    "\n",
    "def select_model_action(\n",
    "    model: DQNAgent, state: torch.tensor, eps: float\n",
    "):\n",
    "    sample = random.random()\n",
    "    if sample > eps:\n",
    "        return model.act(state)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(0, 9)]])\n",
    "    \n",
    "\n",
    "for step in range(n_steps):\n",
    "    t = np.clip(step / eps_steps, 0, 1)\n",
    "    eps = (1-t) * eps_start + t * eps_end\n",
    "        \n",
    "    action = select_model_action(policy_net, state, eps)\n",
    "    next_state, reward = env.step(Action(env._player_1, action.item()))\n",
    "    reward = torch.tensor([reward])\n",
    "\n",
    "    #random action to represent player 2\n",
    "    if not env.isDone():\n",
    "        next_state, _ = env.step(Action(env._player_2, select_dummy_action(next_state)))\n",
    "    else: \n",
    "        next_state = None\n",
    "\n",
    "\n",
    "    memory.push(state, action, next_state, reward)\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    # Go through memory and make updates based on that\n",
    "    optimize_model(\n",
    "        optimizer=optimizer,\n",
    "        policy=policy_net,\n",
    "        target=target_net,\n",
    "        memory=memory,\n",
    "        batch_size=batch_size,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "\n",
    "    if env.isDone():\n",
    "        state = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "Player 1s turn:\n",
      "X| | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "Player 2s turn:\n",
      "X| | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "Player 1s turn:\n",
      "X| | \n",
      "-----\n",
      "X| | \n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "Player 2s turn:\n",
      "X|O| \n",
      "-----\n",
      "X| | \n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "Player 1s turn:\n",
      "X|O| \n",
      "-----\n",
      "X|X| \n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "Player 2s turn:\n",
      "X|O| \n",
      "-----\n",
      "X|X|O\n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "Player 1s turn:\n",
      "X|O|X\n",
      "-----\n",
      "X|X|O\n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "Player 2s turn:\n",
      "X|O|X\n",
      "-----\n",
      "X|X|O\n",
      "-----\n",
      "O| | \n",
      "\n",
      "\n",
      "Player 1s turn:\n",
      "X|O|X\n",
      "-----\n",
      "X|X|O\n",
      "-----\n",
      "O|X| \n",
      "\n",
      "\n",
      "Player 2s turn:\n",
      "X|O|X\n",
      "-----\n",
      "X|X|O\n",
      "-----\n",
      "O|X|O\n",
      "\n",
      "\n",
      "Winner: 0\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToe()\n",
    "\n",
    "user = UserAgent()\n",
    "rand = RandomAgent()\n",
    "ai = policy_net\n",
    "\n",
    "agents = [ai, user]\n",
    "\n",
    "game.render()\n",
    "print(\"\\n\")\n",
    "\n",
    "while not game.isDone():\n",
    "    i = game.summary[\"moves\"]\n",
    "    print(f\"Player {i%2+1}s turn:\")\n",
    "\n",
    "    if i % 2 == 0:\n",
    "        move = agents[0].act(game._one_hot_summary())\n",
    "    else:\n",
    "        move = agents[1].act(game._one_hot_summary())\n",
    "\n",
    "    action = Action(i%2+1, move.item())\n",
    "\n",
    "    game.step(Action(i%2+1, move.item()))\n",
    "    game.render()\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(f\"Winner: {game.summary['winner']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
