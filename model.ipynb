{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = 1.0\n",
    "loss = -1.0\n",
    "draw = -1.0\n",
    "\n",
    "rows = 3\n",
    "cols = 3\n",
    "\n",
    "x = 1\n",
    "o = 2\n",
    "empty = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(move):\n",
    "    return torch.sum(torch.abs(move * torch.rand_like(move)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeNetworkA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TicTacToeNetworkA, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(81, 8000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8000, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 9),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "modelA = TicTacToeNetworkA()\n",
    "optimizerA = optim.Adam(modelA.parameters(), lr=0.001)\n",
    "criterionA = custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeNetworkB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TicTacToeNetworkB, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(81, 8000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8000, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 9),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "modelB = TicTacToeNetworkB()\n",
    "optimizerB = optim.Adam(modelA.parameters(), lr=0.001)\n",
    "criterionB = custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user(game):\n",
    "    while True:\n",
    "        row = int(input(f'Enter row (0-{rows-1}): '))\n",
    "        if row < rows and row >= 0:\n",
    "            break\n",
    "\n",
    "    while True:\n",
    "        col = int(input(f'Enter col (0-{cols-1}): '))\n",
    "        if col < cols and col >= 0:\n",
    "            break\n",
    "\n",
    "    board = torch.ones(rows,cols)\n",
    "    board = board * empty\n",
    "\n",
    "    board[row][col] = 1\n",
    "\n",
    "    return board.view(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkForWin(board):\n",
    "    for row in board:\n",
    "        if torch.all(row == x):\n",
    "            return x\n",
    "        elif torch.all(row == o):\n",
    "            return o\n",
    "\n",
    "    for col in range(0, cols):\n",
    "        col = board[:,col]\n",
    "        if torch.all(col == x):\n",
    "            return x\n",
    "        elif torch.all(col == o):\n",
    "            return o\n",
    "        \n",
    "    truth = board == o\n",
    "    if truth[0][0] and truth[1][1] and truth[2][2]:\n",
    "        return o\n",
    "    elif truth[0][2] and truth[1][1] and truth[2][0]:\n",
    "        return o\n",
    "    \n",
    "    truth = board == x\n",
    "    if truth[0][0] and truth[1][1] and truth[2][2]:\n",
    "        return x\n",
    "    elif truth[0][2] and truth[1][1] and truth[2][0]:\n",
    "        return x\n",
    "        \n",
    "    return empty\n",
    "\n",
    "def print_tic_tac_toe_board(board):\n",
    "    for row in range(3):\n",
    "        # Print horizontal lines between rows\n",
    "        if row != 0:\n",
    "            print(\"-\" * cols  * 3)\n",
    "\n",
    "        for col in range(3):\n",
    "            # Print vertical lines between columns\n",
    "            if col != 0:\n",
    "                print(\"|\", end=\" \")\n",
    "\n",
    "            # Print the cell value\n",
    "            if board[row][col] == 0:\n",
    "                print(\" \", end=\" \")\n",
    "            elif board[row][col] == 1:\n",
    "                print(\"X\", end=\" \")\n",
    "            elif board[row][col] == 2:\n",
    "                print(\"O\", end=\" \")\n",
    "\n",
    "        print()  # Move to the next line after printing each row\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "def run_game(playerA, playerB):\n",
    "    game = torch.ones(rows*cols, rows,cols)\n",
    "    game = game * empty\n",
    "    movesA = []\n",
    "    movesB = []\n",
    "\n",
    "    for i in range(0, rows * cols):\n",
    "        icon = empty\n",
    "        \n",
    "        if (i % 2) == 0:\n",
    "            move = playerA(game)\n",
    "            movesA.append(move)\n",
    "            icon = x\n",
    "        else:\n",
    "            move = playerB(game)\n",
    "            movesB.append(move)\n",
    "            icon = o\n",
    "\n",
    "        row, col = interprateMove(move)\n",
    "\n",
    "        if game[i][row][col] == empty:\n",
    "            for j in range(i,rows*cols):\n",
    "                game[j][row][col] = icon\n",
    "\n",
    "\n",
    "        print_tic_tac_toe_board(game[i])\n",
    "\n",
    "        if checkForWin(game[i]):\n",
    "            return checkForWin(game[i]), movesA, movesB\n",
    "        \n",
    "    return checkForWin(game[i]), movesA, movesB\n",
    "\n",
    "\n",
    "def interprateMove(move):\n",
    "    probs = F.softmax(move, dim=1)\n",
    "    flat_indices = torch.argmax(probs.view(-1))\n",
    "    original_indices = torch.unravel_index(flat_indices, move.shape)\n",
    "    _, flat_index = original_indices\n",
    "    row = math.floor(flat_index / rows)\n",
    "    col = (flat_index % rows).item()\n",
    "    return row, col\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X |   |   \n",
      "---------\n",
      "  |   |   \n",
      "---------\n",
      "  |   |   \n",
      "\n",
      "X |   |   \n",
      "---------\n",
      "  |   |   \n",
      "---------\n",
      "  |   | O \n",
      "\n",
      "X |   | X \n",
      "---------\n",
      "  |   |   \n",
      "---------\n",
      "  |   | O \n",
      "\n",
      "X |   | X \n",
      "---------\n",
      "  |   |   \n",
      "---------\n",
      "O |   | O \n",
      "\n",
      "X |   | X \n",
      "---------\n",
      "  |   |   \n",
      "---------\n",
      "O |   | O \n",
      "\n",
      "X |   | X \n",
      "---------\n",
      "  |   |   \n",
      "---------\n",
      "O |   | O \n",
      "\n",
      "X |   | X \n",
      "---------\n",
      "  |   |   \n",
      "---------\n",
      "O |   | O \n",
      "\n",
      "X |   | X \n",
      "---------\n",
      "  |   |   \n",
      "---------\n",
      "O |   | O \n",
      "\n",
      "X |   | X \n",
      "---------\n",
      "  |   |   \n",
      "---------\n",
      "O |   | O \n",
      "\n",
      "[tensor([[0.1131, 0.1108, 0.1125, 0.1094, 0.1089, 0.1105, 0.1121, 0.1111, 0.1116]],\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[0.1179, 0.1195, 0.1234, 0.1032, 0.1097, 0.1041, 0.1113, 0.1020, 0.1088]],\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[0.1181, 0.1166, 0.1220, 0.1020, 0.1096, 0.1115, 0.1091, 0.1046, 0.1064]],\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[0.1181, 0.1166, 0.1220, 0.1020, 0.1096, 0.1115, 0.1091, 0.1046, 0.1064]],\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[0.1181, 0.1166, 0.1220, 0.1020, 0.1096, 0.1115, 0.1091, 0.1046, 0.1064]],\n",
      "       grad_fn=<SoftmaxBackward0>)]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 81]] is at version 30; expected version 17 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[233], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m movesA_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(movesA)\n\u001b[0;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterionA(movesA_tensor)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m     10\u001b[0m optimizerA\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[0;32m     12\u001b[0m optimizerA\u001b[38;5;241m.\u001b[39mzero_grad() \n",
      "File \u001b[1;32mc:\\Users\\lewibs\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lewibs\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 81]] is at version 30; expected version 17 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0,100):\n",
    "    winner, movesA, movesB = run_game(modelA, modelB)\n",
    "\n",
    "    # how would i then update the model based on all the moves so that it learns?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 0 is the winner!\n"
     ]
    }
   ],
   "source": [
    "#winner = run_game(user, modelA)\n",
    "\n",
    "print(f\"player {winner} is the winner!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
